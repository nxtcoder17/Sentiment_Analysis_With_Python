{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(STOP_WORDS)\n",
    "        self.stop_words.update(string.punctuation)\n",
    "        self.stop_words.remove('not')\n",
    "        \n",
    "        self.docs = []\n",
    "        self.splits = []\n",
    "        \n",
    "    def split_into_sents(self, review):\n",
    "        if not isinstance(review, spacy.tokens.doc.Doc):\n",
    "            review = self.nlp(review)\n",
    "        \n",
    "        sents = []\n",
    "        for sentence in review.sents:\n",
    "            start = 0\n",
    "            counter = 0\n",
    "            print(\"Sentence: \", sentence)\n",
    "            for token in sentence:\n",
    "                # 89 -> Conjunctions,\n",
    "                # 97 -> Punctuations\n",
    "                if token.pos in [89, 97] or token.text.strip() == ',':\n",
    "                    if counter > start: \n",
    "                        sents.append(sentence[start: counter])\n",
    "                    start = counter + 1\n",
    "                counter += 1\n",
    "        return sents\n",
    "    \n",
    "    def feature_extraction(self, custom_sent):\n",
    "        features = {}\n",
    "        \n",
    "        nouns = []\n",
    "        verbs = []\n",
    "        adj = []\n",
    "        \n",
    "        # 92 -> NOUN, 96 -> Proper Noun\n",
    "        # 95 -> PRONOUN\n",
    "        # 86 -> AdVerb\n",
    "        # 84 -> Adjective\n",
    "        # 100 -> VERB\n",
    "        # 87 -> AUX. VERB\n",
    "        # 94 -> Partition (mostly used alongside AUX. VERB)\n",
    "        for token in custom_sent:\n",
    "            if token.pos in [92, 96]:\n",
    "                nouns.append(token.lemma_)\n",
    "            elif token.pos in [84, 86]:\n",
    "                adj.append(token.lemma_)\n",
    "            elif token.pos in [100, 87, 94]:\n",
    "#                 if token.pos == 94 and len(verbs) > 0:\n",
    "#                     verbs[-1] += token.lemma_\n",
    "#                 else:\n",
    "                    verbs.append(token.lemma_)\n",
    "        return { \n",
    "            \"entity\": ', '.join(nouns),\n",
    "            \"features\": ' '.join(adj) if len(adj) > 0 else ' '.join(verbs)\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  I liked the food, but service was awful.\n",
      "Sentence:  Ambience was damn poor.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[I liked the food, service was awful, Ambience was damn poor]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "p.split_into_sents(\"I liked the food, but service was awful. Ambience was damn poor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  I liked the food, but didn't like the service.\n",
      "Sentence:  Ambience was damn poor.\n",
      "{'entity': 'food', 'features': 'like'}\n",
      "{'entity': 'service', 'features': 'do not like'}\n",
      "{'entity': 'Ambience', 'features': 'damn poor'}\n"
     ]
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "sents = p.split_into_sents(\"I liked the food, but didn't like the service. Ambience was damn poor.\")\n",
    "for sent in sents:\n",
    "    print(p.feature_extraction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.3612}\n",
      "{'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.7003}\n"
     ]
    }
   ],
   "source": [
    "print(sia.polarity_scores(\"like\"))\n",
    "print(sia.polarity_scores(\"do  like\"))\n",
    "print(sia.polarity_scores('damn poor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "I -PRON- 95 PRON pronoun nsubj\n",
      "did do 87 AUX auxiliary aux\n",
      "n't not 94 PART particle neg\n",
      "like like 100 VERB verb ROOT\n",
      "the the 90 DET determiner det\n",
      "food food 92 NOUN noun dobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I didn't like the food\")\n",
    "print(type(doc))\n",
    "for token in doc:\n",
    "    print(token,token.lemma_, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- 95 PRON pronoun nsubj\n",
      "did do 87 AUX auxiliary aux\n",
      "not not 94 PART particle neg\n",
      "like like 100 VERB verb ROOT\n",
      "the the 90 DET determiner det\n",
      "food food 92 NOUN noun dobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I did not like the food\")\n",
    "for token in doc:\n",
    "    print(token, token.lemma_, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food 92 NOUN noun nsubj\n",
      "was 87 AUX auxiliary ROOT\n",
      "pretty 86 ADV adverb advmod\n",
      "bad 84 ADJ adjective acomp\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"food was pretty bad\")\n",
    "for token in doc:\n",
    "    print(token, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food 92 NOUN noun nsubj\n",
      "was 87 AUX auxiliary ROOT\n",
      "good 84 ADJ adjective acomp\n",
      ", 97 PUNCT punctuation punct\n",
      "but 89 CCONJ coordinating conjunction cc\n",
      "Ambience 96 PROPN proper noun nsubj\n",
      "was 87 AUX auxiliary conj\n",
      "pretty 86 ADV adverb advmod\n",
      "awful 84 ADJ adjective acomp\n",
      ". 97 PUNCT punctuation punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"food was good, but Ambience was pretty awful.\")\n",
    "for token in doc:\n",
    "    print(token, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "p.split_into_sents(\"I didn't like the food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

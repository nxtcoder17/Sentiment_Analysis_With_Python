{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(STOP_WORDS)\n",
    "        self.stop_words.update(string.punctuation)\n",
    "        self.stop_words.remove('not')\n",
    "        \n",
    "        self.docs = []\n",
    "        self.splits = []\n",
    "        \n",
    "    def split_into_sents(self, review):\n",
    "        if not isinstance(review, spacy.tokens.doc.Doc):\n",
    "            review = self.nlp(' '.join([r.lower() for r in review.split(' ')]))\n",
    "#             review = self.nlp(' '.join([r.lower() for r in review.split(' ') if r.lower() not in self.stop_words]))\n",
    "        \n",
    "        sents = []\n",
    "        for sentence in review.sents:\n",
    "            start = 0\n",
    "            counter = 0\n",
    "            for token in sentence:\n",
    "                # 89 -> Conjunctions,\n",
    "                # 97 -> Punctuations\n",
    "                # if token.pos in [89, 97] or token.text.strip() == ',':\n",
    "                if token.pos in [89, 97]:\n",
    "                    if counter > start: \n",
    "                        sents.append(sentence[start: counter])\n",
    "                    start = counter + 1\n",
    "                counter += 1\n",
    "            if counter > start:\n",
    "                sents.append(sentence[start: counter])\n",
    "        return sents\n",
    "    \n",
    "    def lemmitize(self, sentence):\n",
    "        return ' '.join([x.lemma_ for x in self.nlp(sentence) if x.text.lower() not in self.stop_words])\n",
    "        # return ' '.join([x.lemma_ for x in self.nlp(sentence)])\n",
    "    \n",
    "    def feature_extraction(self, custom_sent):\n",
    "        features = {}\n",
    "        \n",
    "        nouns = []\n",
    "        adjs = []\n",
    "        verbs = []\n",
    "        intjs = []\n",
    "        verbsAdjIntjs = []\n",
    "        \n",
    "        # 92 -> NOUN, 96 -> Proper Noun\n",
    "        # 95 -> PRONOUN\n",
    "        # 86 -> AdVerb\n",
    "        # 84 -> Adjective\n",
    "        # 100 -> VERB\n",
    "        # 87 -> AUX. VERB\n",
    "        # 94 -> Partition (mostly used alongside AUX. VERB)\n",
    "        # 91 -> Interjection, like Wow, Alas, Hurraydark\n",
    "        for token in custom_sent:\n",
    "            if token.pos in [92, 96]:\n",
    "                nouns.append(token.lemma_)\n",
    "            elif token.pos in [84, 86, 100, 87, 94, 91]:\n",
    "                verbsAdjIntjs.append(token.lemma_)\n",
    "#             elif token.pos in [84, 86, 91]:\n",
    "#                 adjs.append(token.lemma_)\n",
    "#             elif token.pos in [87, 100, 94]:\n",
    "#                 verbs.append(token.lemma_)\n",
    "\n",
    "        return { \n",
    "            \"entity\": ', '.join(nouns),\n",
    "             \"features\": \" \".join(verbsAdjIntjs),\n",
    "#               \"features\": ' '.join(adjs) + ' '.join(verbs)\n",
    "#                 \"features\": ' '.join(adjs) if len(adjs) > 0 else ' '.join(verbs)\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBModel:\n",
    "    def __init__(self, X=None, Y=None, debug=False, file_path=\"nb-darkmodel.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = MultinomialNB()\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel:\n",
    "    def __init__(self, X=None, Y=None, debug=False, file_path=\"random-forest-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = RandomForestClassifier(max_depth=25, random_state=17)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorModel:\n",
    "    def __init__(self, X=None, Y=None, debug=False, file_path=\"svm-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = svm.LinearSVC()\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighborsModel:\n",
    "    def __init__(self, X=None, Y=None, N=20, debug=False, file_path=\"knn-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = KNeighborsClassifier(N, weights='uniform')\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/Restaurant_Reviews.tsv\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: Liked, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:]['Liked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:]['Review']\n",
    "Y = df.iloc[:]['Liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Wow... Loved this place.\n",
       "1                                   Crust is not good.\n",
       "2            Not tasty and the texture was just nasty.\n",
       "3    Stopped by during the late May bank holiday of...\n",
       "4    The selection on the menu was great and so wer...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PreProcessor()\n",
    "lemma_X = [p.lemmitize(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow ... love place',\n",
       " 'crust not good',\n",
       " 'not tasty texture nasty',\n",
       " 'stop late bank holiday Rick Steve recommendation love',\n",
       " 'selection menu great price']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: Liked, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7966666666666666\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80       143\n",
      "           1       0.83      0.76      0.80       157\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.80      0.80      0.80       300\n",
      "weighted avg       0.80      0.80      0.80       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = NBModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7333333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.92      0.77       143\n",
      "           1       0.89      0.56      0.69       157\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.77      0.74      0.73       300\n",
      "weighted avg       0.78      0.73      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7933333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80       143\n",
      "           1       0.87      0.71      0.78       157\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.80      0.80      0.79       300\n",
      "weighted avg       0.81      0.79      0.79       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sv_model = SupportVectorModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.90      0.80       143\n",
      "           1       0.89      0.69      0.77       157\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.80      0.79      0.79       300\n",
      "weighted avg       0.81      0.79      0.79       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNearestNeighborsModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint(sentence):\n",
    "    def parse_result(resultArr):\n",
    "        return \"POSITIVE\" if resultArr[0] == 1 else \"NEGATIVE\"\n",
    "    \n",
    "    RJUST = 30\n",
    "    for sent in p.split_into_sents(sentence):\n",
    "        print(\"\\n[Custom Sentence]: \", sent, '\\n')\n",
    "        print(\"[MultinomialNB]:\".rjust(40), parse_result(nb_model.predict(str(sent))))\n",
    "        print(\"[Random Forest Model]:\".rjust(40), parse_result(forest_model.predict(str(sent))))\n",
    "        print(\"[Support Vector Model]:\".rjust(40), parse_result(sv_model.predict(str(sent))))\n",
    "        print(\"[K Nearest Neighbors Model]:\".rjust(40), parse_result(knn_model.predict(str(sent))))\n",
    "        \n",
    "        print(f\"\\n{p.feature_extraction(sent)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I did not like the food\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "i did not like the food\n",
      "{'entity': 'food', 'features': 'do not like'}\n"
     ]
    }
   ],
   "source": [
    "sent = 'I did not like the food'\n",
    "print(sent)\n",
    "print(nb_model.predict(sent))\n",
    "print(forest_model.predict(sent))\n",
    "print(sv_model.predict(sent))\n",
    "print(knn_model.predict(sent))\n",
    "\n",
    "p = PreProcessor()\n",
    "for _sent in p.split_into_sents(sent):\n",
    "        print(_sent)\n",
    "        print(p.feature_extraction(_sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Custom Sentence]:  i did not like the food \n",
      "\n",
      "                           MultinomialNB NEGATIVE\n",
      "                     Random Forest Model NEGATIVE\n",
      "                  Support Vector Model:  NEGATIVE\n",
      "             K Nearest Neighbors Model:  NEGATIVE\n",
      "\n",
      "{'entity': 'food', 'features': 'do not like'}\n"
     ]
    }
   ],
   "source": [
    "pprint(\"I did not like the food.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Custom Sentence]:  i liked the food \n",
      "\n",
      "       MultinomialNB POSITIVE\n",
      " Random Forest Model NEGATIVE\n",
      "Support Vector Model:  NEGATIVE\n",
      "K Nearest Neighbors Model:  POSITIVE\n",
      "\n",
      "{'entity': 'food', 'features': 'like'}\n",
      "\n",
      "[Custom Sentence]:  service was awful \n",
      "\n",
      "       MultinomialNB NEGATIVE\n",
      " Random Forest Model NEGATIVE\n",
      "Support Vector Model:  NEGATIVE\n",
      "K Nearest Neighbors Model:  NEGATIVE\n",
      "\n",
      "{'entity': 'service', 'features': 'be awful'}\n",
      "\n",
      "[Custom Sentence]:  ambience was damn poor \n",
      "\n",
      "       MultinomialNB NEGATIVE\n",
      " Random Forest Model NEGATIVE\n",
      "Support Vector Model:  NEGATIVE\n",
      "K Nearest Neighbors Model:  NEGATIVE\n",
      "\n",
      "{'entity': 'ambience', 'features': 'be damn poor'}\n"
     ]
    }
   ],
   "source": [
    "pprint('I liked the food, but service was awful. Ambience was damn poor.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

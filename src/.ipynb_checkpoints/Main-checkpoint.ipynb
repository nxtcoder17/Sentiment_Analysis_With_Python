{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import numpy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "        self.stop_words = set(STOP_WORDS)\n",
    "        self.stop_words.update(string.punctuation)\n",
    "        self.stop_words.remove('not')\n",
    "        \n",
    "        self.docs = []\n",
    "        self.splits = []\n",
    "        \n",
    "    def split_into_sents(self, review):\n",
    "        if not isinstance(review, spacy.tokens.doc.Doc):\n",
    "            review = self.nlp(' '.join([r.lower() for r in review.split(' ') if r.lower() not in self.stop_words]))\n",
    "        \n",
    "        sents = []\n",
    "        for sentence in review.sents:\n",
    "            start = 0\n",
    "            counter = 0\n",
    "            for token in sentence:\n",
    "                # 89 -> Conjunctions,\n",
    "                # 97 -> Punctuations\n",
    "                # if token.pos in [89, 97] or token.text.strip() == ',':\n",
    "                if token.pos in [89, 97]:\n",
    "                    if counter > start: \n",
    "                        sents.append(sentence[start: counter])\n",
    "                    start = counter + 1\n",
    "                counter += 1\n",
    "            if counter > start:\n",
    "                sents.append(sentence[start: counter])\n",
    "        return sents\n",
    "    \n",
    "    def lemmitize(self, sentence):\n",
    "        return ' '.join([x.lemma_ for x in self.nlp(sentence) if x.text.lower() not in self.stop_words])\n",
    "        # return ' '.join([x.lemma_ for x in self.nlp(sentence)])\n",
    "    \n",
    "    def feature_extraction(self, custom_sent):\n",
    "        features = {}\n",
    "        \n",
    "        nouns = []\n",
    "        adjs = []\n",
    "        verbs = []\n",
    "        intjs = []\n",
    "        verbsAdjIntjs = []\n",
    "        \n",
    "        # 92 -> NOUN, 96 -> Proper Noun\n",
    "        # 95 -> PRONOUN\n",
    "        # 86 -> AdVerb\n",
    "        # 84 -> Adjective\n",
    "        # 100 -> VERB\n",
    "        # 87 -> AUX. VERB\n",
    "        # 94 -> Partition (mostly used alongside AUX. VERB)\n",
    "        # 91 -> Interjection, like Wow, Alas, Hurray\n",
    "        for token in custom_sent:\n",
    "            if token.pos in [92, 96]:\n",
    "                nouns.append(token.lemma_)\n",
    "#             elif token.pos in [84, 86, 100, 87, 94, 91]:\n",
    "#                 verbsAdjIntjs.append(token.lemma_)\n",
    "            elif token.pos in [84, 86, 91]:\n",
    "                adjs.append(token.lemma_)\n",
    "            elif token.pos in [87, 100, 94]:\n",
    "                verbs.append(token.lemma_)\n",
    "                \n",
    "        return { \n",
    "            \"entity\": ', '.join(nouns),\n",
    "#             \"features\": \" \".join(verbsAdjIntjs),\n",
    "#              \"features\": ' '.join(adjs) + ' '.join(verbs)\n",
    "                \"features\": ' '.join(adjs) if len(adjs) > 0 else ' '.join(verbs)\n",
    "        }\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBModel:\n",
    "    def __init__(self, X=None, Y=None, debug=False, file_path=\"trained-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = MultinomialNB()\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestModel:\n",
    "    def __init__(self, X=None, Y=None, debug=False, file_path=\"random-forest-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = RandomForestClassifier(max_depth=25, random_state=17)\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupportVectorModel:\n",
    "    def __init__(self, X=None, Y=None, debug=False, file_path=\"random-forest-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = svm.LinearSVC()\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNearestNeighborsModel:\n",
    "    def __init__(self, X=None, Y=None, N=20, debug=False, file_path=\"random-forest-model.pkl\", force=False):\n",
    "        self.vectorizer = None,\n",
    "        self.model = None,\n",
    "        if not force and Path.exists(Path(file_path)):\n",
    "            self.vectorizer, self.model = pickle.load(open(file_path, \"rb\"))\n",
    "        else:\n",
    "            self.vectorizer = TfidfVectorizer(ngram_range=(1, 3), strip_accents='unicode')\n",
    "            X = self.vectorizer.fit_transform(X)\n",
    "            self.model = KNeighborsClassifier(N, weights='uniform')\n",
    "            x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=17)\n",
    "            \n",
    "            # Training Model\n",
    "            self.model.fit(x_train, y_train)\n",
    "            \n",
    "            # Training Results stats\n",
    "            predicted = self.model.predict(x_test)\n",
    "            if debug:\n",
    "                print(f\"Accuracy: {accuracy_score(y_test, predicted)}\")\n",
    "                print(f\"Classification Report:\")\n",
    "                print(classification_report(y_test, predicted))\n",
    "                \n",
    "            pickled_tuple = (self.vectorizer, self.model)\n",
    "            pickle.dump(pickled_tuple, open(file_path, 'wb'))\n",
    "            \n",
    "    def predict(self, test):\n",
    "        if not isinstance(test, pd.Series):\n",
    "            test = pd.Series([test])\n",
    "        test = self.vectorizer.transform(test)\n",
    "        return self.model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../datasets/Restaurant_Reviews.tsv\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    500\n",
       "0    500\n",
       "Name: Liked, dtype: int64"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:]['Liked'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:]['Review']\n",
    "Y = df.iloc[:]['Liked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                             Wow... Loved this place.\n",
       "1                                   Crust is not good.\n",
       "2            Not tasty and the texture was just nasty.\n",
       "3    Stopped by during the late May bank holiday of...\n",
       "4    The selection on the menu was great and so wer...\n",
       "Name: Review, dtype: object"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PreProcessor()\n",
    "lemma_X = [p.lemmitize(x) for x in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wow ... love place',\n",
       " 'crust not good',\n",
       " 'not tasty texture nasty',\n",
       " 'stop late bank holiday Rick Steve recommendation love',\n",
       " 'selection menu great price']"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    0\n",
       "3    1\n",
       "4    1\n",
       "Name: Liked, dtype: int64"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7966666666666666\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.83      0.80       143\n",
      "           1       0.83      0.76      0.80       157\n",
      "\n",
      "    accuracy                           0.80       300\n",
      "   macro avg       0.80      0.80      0.80       300\n",
      "weighted avg       0.80      0.80      0.80       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_model = NBModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7333333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.92      0.77       143\n",
      "           1       0.89      0.56      0.69       157\n",
      "\n",
      "    accuracy                           0.73       300\n",
      "   macro avg       0.77      0.74      0.73       300\n",
      "weighted avg       0.78      0.73      0.73       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forest_model = RandomForestModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7933333333333333\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.88      0.80       143\n",
      "           1       0.87      0.71      0.78       157\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.80      0.80      0.79       300\n",
      "weighted avg       0.81      0.79      0.79       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sv_model = SupportVectorModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.79\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.90      0.80       143\n",
      "           1       0.89      0.69      0.77       157\n",
      "\n",
      "    accuracy                           0.79       300\n",
      "   macro avg       0.80      0.79      0.79       300\n",
      "weighted avg       0.81      0.79      0.79       300\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNearestNeighborsModel(X=lemma_X, Y=Y, debug=True, force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I did not like the food\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sent = 'I did not like the food'\n",
    "print(sent)\n",
    "print(nb_model.predict(sent))\n",
    "print(forest_model.predict(sent))\n",
    "print(sv_model.predict(sent))\n",
    "print(knn_model.predict(sent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not like food\n",
      "[0]\n",
      "[0]\n",
      "[0]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "sent = p.lemmitize('I did not like the food')\n",
    "print(sent)\n",
    "print(nb_model.predict(sent))\n",
    "print(forest_model.predict(sent))\n",
    "print(sv_model.predict(sent))\n",
    "print(knn_model.predict(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[liked food, service awful, ambience damn poor]"
      ]
     },
     "execution_count": 343,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "p.split_into_sents(\"I liked the food, but service was awful. Ambience was damn poor.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  honeslty didn't taste fresh.)\n",
      "token:  . 97 PUNCT\n",
      "token:  ) 97 PUNCT\n",
      "[honeslty didn't taste fresh] 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[honeslty didn't taste fresh]"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "p.split_into_sents(\"Honeslty it didn't taste THAT fresh.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'texture', 'features': 'tasty nastynot'}\n"
     ]
    }
   ],
   "source": [
    "sents = p.split_into_sents(X[2])\n",
    "for sent in sents:\n",
    "    print(p.feature_extraction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service was very prompt.\n",
      "{'entity': 'service, prompt', 'features': ''}\n",
      "Would not go back.\n",
      "{'entity': '', 'features': 'not back'}\n",
      "The cashier had no care what so ever on what I had to say it still ended up being wayyy overpriced.\n",
      "{'entity': 'cashier, care, wayyy', 'features': 'end overprice'}\n",
      "I tried the Cape Cod ravoli, chicken, with cranberry...mmmm!\n",
      "{'entity': 'cape, cod, ravoli', 'features': 'try'}\n",
      "I tried the Cape Cod ravoli, chicken, with cranberry...mmmm!\n",
      "{'entity': 'chicken', 'features': ''}\n",
      "I tried the Cape Cod ravoli, chicken, with cranberry...mmmm!\n",
      "{'entity': 'cranberry', 'features': ''}\n",
      "I tried the Cape Cod ravoli, chicken, with cranberry...mmmm!\n",
      "{'entity': 'mmmm', 'features': ''}\n",
      "I was disgusted because I was pretty sure that was human hair.\n",
      "{'entity': 'hair', 'features': 'disgusted pretty sure human'}\n",
      "I was shocked because no signs indicate cash only.\n",
      "{'entity': 'sign, cash', 'features': 'shocked indicate only'}\n",
      "Highly recommended.\n",
      "{'entity': '', 'features': 'highly recommend'}\n",
      "Waitress was a little slow in service.\n",
      "{'entity': 'service', 'features': 'waitress little slow'}\n",
      "This place is not worth your time, let alone Vegas.\n",
      "{'entity': 'place, time', 'features': 'not worth'}\n",
      "This place is not worth your time, let alone Vegas.\n",
      "{'entity': 'vegas', 'features': 'let'}\n",
      "did not like at all.\n",
      "{'entity': '', 'features': 'not like'}\n"
     ]
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "features = []\n",
    "for x in X[10:20]:\n",
    "    for sent in p.split_into_sents(x):\n",
    "        print(x)\n",
    "        print(p.feature_extraction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'entity': 'food', 'features': 'like'}\n",
      "{'entity': 'service', 'features': 'do not like'}\n",
      "{'entity': 'Ambience', 'features': 'be damn poor'}\n"
     ]
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "sents = p.split_into_sents(\"I liked the food, but didn't like the service. Ambience was damn poor.\")\n",
    "for sent in sents:\n",
    "    print(p.feature_extraction(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sia = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.3612}\n",
      "{'neg': 0.513, 'neu': 0.487, 'pos': 0.0, 'compound': -0.2755}\n",
      "{'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.7003}\n"
     ]
    }
   ],
   "source": [
    "print(sia.polarity_scores(\"like\"))\n",
    "print(sia.polarity_scores(\"do not like\"))\n",
    "print(sia.polarity_scores('damn poor'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.doc.Doc'>\n",
      "I -PRON- 95 PRON pronoun nsubj\n",
      "did do 87 AUX auxiliary aux\n",
      "n't not 94 PART particle neg\n",
      "like like 100 VERB verb ROOT\n",
      "the the 90 DET determiner det\n",
      "food food 92 NOUN noun dobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I didn't like the food\")\n",
    "print(type(doc))\n",
    "for token in doc:\n",
    "    print(token,token.lemma_, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -PRON- 95 PRON pronoun nsubj\n",
      "did do 87 AUX auxiliary aux\n",
      "not not 94 PART particle neg\n",
      "like like 100 VERB verb ROOT\n",
      "the the 90 DET determiner det\n",
      "food food 92 NOUN noun dobj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"I did not like the food\")\n",
    "for token in doc:\n",
    "    print(token, token.lemma_, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food 92 NOUN noun nsubj\n",
      "was 87 AUX auxiliary ROOT\n",
      "pretty 86 ADV adverb advmod\n",
      "bad 84 ADJ adjective acomp\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"food was pretty bad\")\n",
    "for token in doc:\n",
    "    print(token, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "food 92 NOUN noun nsubj\n",
      "was 87 AUX auxiliary ROOT\n",
      "good 84 ADJ adjective acomp\n",
      ", 97 PUNCT punctuation punct\n",
      "but 89 CCONJ coordinating conjunction cc\n",
      "Ambience 96 PROPN proper noun nsubj\n",
      "was 87 AUX auxiliary conj\n",
      "pretty 86 ADV adverb advmod\n",
      "awful 84 ADJ adjective acomp\n",
      ". 97 PUNCT punctuation punct\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"food was good, but Ambience was pretty awful.\")\n",
    "for token in doc:\n",
    "    print(token, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[I didn't like the food]"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = PreProcessor()\n",
    "p.split_into_sents(\"I didn't like the food\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both 90 DET determiner det\n",
      "Veg 96 PROPN proper noun nmod\n",
      "and 89 CCONJ coordinating conjunction cc\n",
      "Non 96 PROPN proper noun dep\n",
      "- 84 ADJ adjective dep\n",
      "Veg 84 ADJ adjective compound\n",
      "Items 92 NOUN noun nsubj\n",
      "were 87 AUX auxiliary ROOT\n",
      "great 84 ADJ adjective acomp\n",
      ". 97 PUNCT punctuation punct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.594, 'pos': 0.406, 'compound': 0.6249}"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"Both Veg and Non-Veg Items were great.\"\n",
    "doc = nlp(sent)\n",
    "for token in doc:\n",
    "    print(token, token.pos, token.pos_, spacy.explain(token.pos_), token.dep_)\n",
    "\n",
    "sia.polarity_scores(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Honeslty, it didn't taste THAT fresh]"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.split_into_sents(sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
